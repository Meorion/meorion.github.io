[{"content":"Forecasting Healthcare Revenue with ARIMA Time Series Modeling Description This project aimed to develop a reliable forecasting model to predict monthly revenue for a healthcare organization. Leveraging the SARIMAX model (a more general form of ARIMA) in Python, I analyzed two years of historical daily revenue data. An ARIMA(0, 1, 2) structure within the SARIMAX framework was found to be the most accurate for forecasting future revenue trends.\nSkills Used Python, Pandas, NumPy, ARIMA Modeling, Time Series Analysis, Data Visualization, Statistical Analysis, Data Cleaning \u0026 Transformation\nKey Findings The analysis revealed a weekly seasonality pattern in the data, suggesting potential fluctuations in revenue based on the day of the week. ARIMA (0, 1, 2) model was identified as the most accurate based on the Akaike Information Criterion (AIC) and demonstrated strong predictive performance. The model achieved an RMSE of 3.55521, which given the mean of the test data (16.4148), indicates the model’s forecasets are reasonably accurate in forecasting the next month’s revenue. Visualizations Line Graph With the Trendline\nThis line graph shows the trendline of the data over time, highlighting the overall trend.\nPredictions Compared to Testing Dataset\nThis plot compares the model’s predictions with the actual values from the testing dataset, illustrating the model’s accuracy.\nAutocorrelation Plot\nThis autocorrelation plot shows the correlation of the time series with its own past values, helping to identify patterns and lags.\nRecommendations The healthcare organization should leverage this model to inform budget projections, resource allocation decisions, and strategic planning. Further analysis could explore the drivers of the weekly seasonality patterns to identify opportunities for revenue optimization. Continued collection of daily revenue data will enable further refinement and improvement of the model’s accuracy over time. Link to Code GitHub Repository\n","description":"This project aimed to develop a reliable forecasting model to predict monthly revenue for a healthcare organization.","tags":["ARIMA"],"title":"Forecasting Healthcare Revenue with ARIMA Time Series Modeling","uri":"/post/projects/arima/"},{"content":"Data Dashboard and Storytelling with Tableau: Hospital Readmission Rates Description This project involved developing an interactive Tableau dashboard to explore the impact of state population on hospital readmission rates. The goal was to create a compelling and accessible data visualization that would effectively communicate insights to executive leaders and inform decision-making.\nSkills Used Tableau, Data Visualization, Data Storytelling, LOD, Expressions, Calculated Fields, Dashboard Design, Colorblind-Friendly Design\nKey Findings The analysis revealed that states with larger populations tended to have lower hospital readmission rates. The analysis revealed a potential geographic trend, with higher readmission rates observed in some north-central US states despite lower populations. Further investigation is needed to determine the specific factors driving this trend. Visualizations Line Chart Showing Relationship Between State Population and Readmission Rates This line chart shows the relationship between state population and readmission rates, revealing a negative correlation between the two variables.\nScatterplot Showing Relationship Between State Population and Readmission Rates “This scatterplot shows the relationship between state population and readmission rate, with each point representing a state. The negative trendline highlights the inverse correlation between the two variables.”\nMap of Readmission Rates by State (Choropleth) “This choropleth map displays hospital readmission rates by state, using color intensity to highlight variations in rates across the US.\nInteractive Dashboard “This interactive dashboard provides a comprehensive view of hospital readmission rates, allowing users to explore trends by state, compare metrics, and drill down into specific data points.”\nLink to Tableau Dashboard Public Tableau Online Dashboard\nRecommendations Re-evaluate with Current Data: To gain a more accurate understanding of hospital readmission rates in the current healthcare landscape, the organization should update the analysis with post-COVID data. This will provide insights into how the pandemic may have impacted readmission trends and identify potential new areas of focus. Tailor the Dashboard for Specific Audiences: The interactive nature of the dashboard allows for customization based on the needs of different stakeholders. Healthcare providers might be interested in county-level comparisons within their state, while policymakers might want to explore national trends or regional disparities. By tailoring the dashboard’s presentation and insights to specific audiences, the organization can enhance its impact and drive more informed decision-making. ","description":"This project involved developing an interactive Tableau dashboard to explore the impact of state population on hospital readmission rates.","tags":["tableau","storytelling"],"title":"Data Dashboard and Storytelling with Tableau, Hospital Readmission Rates","uri":"/post/projects/tableau/"},{"content":" PostgreSQL Database Management and Data Acquisition Description This project involved using PostgreSQL and SQL to manage and analyze a dataset of hospital readmissions. My goal was to efficiently prepare the data for further analysis and visualization in Tableau. I developed a custom SQL query to extract, transform, and integrate data from multiple sources, ensuring data accuracy and consistency for a subsequent analysis of hospital readmission rates.\nSkills Used SQL, PostgreSQL, Data Acquisition, Data Cleaning, Data Transformation, Data Extraction Key Findings Effective Data Transformation: Developed a multi-table SQL query to extract specific data points, including [mention key data elements: state, population, readmission rates, etc.], from the database, demonstrating proficiency in complex query writing and data manipulation. Essential Data Acquisition \u0026 Cleaning: Imported data from multiple CSV files, performed data cleaning to address inconsistencies and errors, and validated data integrity, demonstrating proficiency in handling and preparing real-world datasets. Efficient Data Extraction: Developed a multi-table SQL query to extract specific data points, including state, population, readmission rates, from the database, demonstrating proficiency in complex query writing and data manipulation. Visualizations Custom SQL Query This custom SQL query demonstrates my ability to extract and join data from multiple tables to meet specific analysis requirements in Tableau.\nData Source Relationship in Tableau “This visualization, created in Tableau, showcases the results of my SQL data extraction and transformation efforts, demonstrating how I effectively prepared the data for analysis and insights generation.”\nRecommendations Implement SQL constraints (e.g., `CHECK`, `UNIQUE`) to enforce data validation rules and prevent the entry of invalid or duplicate data. Develop a series of SQL scripts to automate the process of importing data from CSV files, performing data cleaning, and validating data integrity, saving time by minimizing any manual effort. ","description":"This project involved using PostgreSQL and SQL to manage and analyze a dataset of hospital readmissions.","tags":["sql","postgres"],"title":"PostgreSQL Database Management and Data Acquisition","uri":"/post/projects/postgres/"},{"content":" Sentiment Analysis of Movie Reviews Using a Feedforward Neural Network Description This project aimed to build a machine learning model capable of predicting the sentiment (positive or negative) of movie reviews. I used a dataset of 3,000 labeled movie reviews and applied natural language processing (NLP) techniques, including text cleaning, tokenization, and padding, to prepare the data for analysis by a feedforward neural network (FFNN).\nSkills Used Python, Pandas, NumPy, TensorFlow, Keras, Natural Language Processing (NLP), Tokenization, Padding, Word Embeddings, Neural Networks, Model Evaluation, Data Visualization\nKey Findings The FFNN model, consisting of an embedding layer followed by three dense layers, achieved an accuracy of 77% on the test dataset, demonstrating its effectiveness in classifying sentiment. Early stopping with a patience of 2 epochs was implemented during training to prevent overfitting, ensuring the model’s ability to generalize well to new data. Visualizations Model Summary\nThe FFNN architecture consists of an embedding layer, three dense layers with 100, 50, and 2 nodes, respectively, and a sigmoid activation function in the output layer.\nModel Accuracy\nThe model’s accuracy on the training and validation datasets increased over epochs, with early stopping preventing overfitting after Epoch 6.\nRecommendations Training the model on a larger, more diverse dataset could improve accuracy and robustness by exposing the model to more variations in language and sentiment expressions. Experimenting with different hyperparameter settings, such as the learning rate or the number of nodes in each layer, could potentially increase the model’s accuracy. Exploring more advanced network architectures, such as recurrent neural networks (RNNs) or transformers, might further enhance performance. Link to Code: Github Repository\nPotential Applications This type of sentiment analysis model has wide applications in business, including analyzing customer feedback, monitoring brand reputation on social media, and understanding product reviews. ","description":"This project aimed to build a machine learning model capable of predicting the sentiment (positive or negative) of movie reviews.","tags":["sentiment analysis","nlp","keras"],"title":"Sentiment Analysis of Movie Reviews Using a Feedforward Neural Network","uri":"/post/projects/sentiment/"},{"content":" Predicting Hospital Length of Stay Using Multiple Linear Regression Description This project aimed to identify the key factors that influence the length of hospital stay for patients. Using a dataset of 10,000 medical records, I built a multiple linear regression model in Python to predict the number of days a patient would be hospitalized based on factors like demographics, medical history, and treatment details.\nSkills Used Python, Pandas, NumPy, Scikit-learn, Statistical Modeling, Multiple Linear Regression, Data Cleaning, Data Transformation (Label Encoding, One-Hot Encoding), Feature Selection (VIF, RFE), Model Evaluation, Data Visualization, Communication of Results.\nKey Findings The initial model, including a wide range of variables, achieved an adjusted R-squared of 0.997, indicating a very strong fit.\nFeature selection using VIF analysis and Recursive Feature Elimination (RFE) resulted in a more parsimonious model with only four key variables: readmission status, complication risk, initial admission type (emergency vs. elective), and average daily charges.\nThe reduced model, while less complex, still maintained a high adjusted R-squared of 0.995, demonstrating that the selected features effectively captured the most influential factors driving hospital length of stay.\nVisualizations Coefficients of the Multiple Linear Regression Model\nInitial_days vs TotalCharge\nThis scatterplot visualizes the relationship between the average daily charges and the initial length of hospital stay, highlighting the positive correlation between these two variables.\nHistogram of Residuals in Reduced Model\nThis histogram displays the distribution of the residuals (the difference between predicted and actual values), indicating that the residuals are approximately normally distributed, which supports the validity of the regression model.\nRecommendations Focus on reducing readmissions: The analysis highlighted readmission status as a strong predictor of longer hospital stays. Healthcare organizations should prioritize efforts to minimize readmission rates through improved discharge planning, patient education, and follow-up care. Optimize average charges per day: While average daily charges were positively correlated with length of stay, further investigation is needed to understand the drivers of those charges and identify opportunities for cost optimization without compromising quality of care. Link to Code: Github Repository\nBusiness Impact By reducing readmission rates and optimizing costs associated with hospital stays, healthcare organizations can potentially achieve significant savings and improve the overall efficiency of care delivery. ","description":"This project aimed to identify the key factors that influence the length of hospital stay for patients.","tags":["regression","scikit-learn"],"title":"Predicting Hospital Length of Stay Using Multiple Linear Regression","uri":"/post/projects/mult-lin-reg/"},{"content":" Predicting Hospital Readmission Using Logistic Regression Description This project aimed to identify the factors influencing the likelihood of a patient being readmitted to the hospital within 30 days of discharge. Using a dataset of 10,000 medical records, I built a logistic regression model in Python to predict readmission probability based on patient demographics, medical history, initial hospital stay length, and other relevant factors.\nSkills Used Python, Pandas, NumPy, Scikit-learn, Logistic Regression, Feature Selection (VIF, Stepwise Selection), Data Cleaning, Data Transformation (Label Encoding, One-Hot Encoding), Model Evaluation (Accuracy, Confusion Matrix, Pseudo R-Squared), Data Visualization, Interpretation of Odds Ratios.\nKey Findings The initial model, encompassing a wide range of variables, achieved a pseudo R-squared of 0.94, indicating strong explanatory power. Through a statistically-driven feature selection process (VIF analysis and stepwise backward elimination), I identified five key predictors of readmission: length of initial hospital stay, anxiety, high blood pressure, and admission type (emergency vs. elective). The reduced model, while simpler, maintained a high pseudo R-squared of 0.93 and achieved an accuracy of 98.1% on the test dataset, demonstrating its ability to effectively predict readmission likelihood. Visualizations Confusion Matrix\nThis confusion matrix visualizes the performance of the logistic regression model on the test dataset. The model correctly classified 589 out of 600 cases, resulting in a high accuracy of 98.1%.\nOdds Ratios for Hospital Readmission\nThis bar chart displays the odds ratios for the significant predictors of hospital readmission within 30 days of discharge. Notably, admission through the Emergency Department significantly increases the odds of readmission (over 6 times higher compared to elective admissions). Longer initial hospital stays also increase the odds of readmission, while having an anxiety diagnosis is associated with a decrease in readmission likelihood.\nRecommendations Focus on Reducing Length of Initial Stay: The length of the initial hospital stay was the strongest predictor of readmission. Healthcare providers should consider strategies to optimize treatment plans and discharge procedures to minimize unnecessary hospital days while ensuring quality care. Targeted Interventions for High-Risk Groups: The model identified high blood pressure and emergency admissions as factors associated with increased readmission risk. Hospitals could implement targeted interventions, such as closer monitoring, follow-up appointments, or patient education programs, to address the specific needs of these groups and reduce their likelihood of readmission. Link to Code Github Repository\n","description":"This project aimed to identify the factors influencing the likelihood of a patient being readmitted to the hospital within 30 days of discharge. ","tags":["regression","scikit-learn"],"title":"Predicting Hospital Readmission Using Logistic Regression","uri":"/post/projects/log-reg/"},{"content":" Identifying Distinct Patient Groups Using K-means Clustering Description This project aimed to segment patients into distinct groups based on key healthcare metrics using K-means clustering, an unsupervised machine learning algorithm. The goal was to identify patterns in patient data that could inform care strategies and improve hospital resource allocation. I applied this technique to a dataset of 10,000 patient records, focusing on continuous variables like income, vitamin D levels, average daily charges, length of hospital stay, and additional daily charges.\nSkills Used Python, Pandas, NumPy, Scikit-learn, K-means Clustering, Data Cleaning, Data Standardization, Cluster Analysis, Silhouette Score, Data Visualization, Interpretation of Cluster Characteristics.\nKey Findings The optimal number of clusters was determined to be two using the elbow method, suggesting two distinct patient profiles within the dataset. Cluster analysis revealed that one cluster represented patients with shorter hospital stays and lower average charges, suggesting less complex medical needs. The second cluster exhibited longer hospital stays and higher average charges, indicating potentially more complex or chronic conditions. The model achieved a Silhouette Score of 0.36, indicating a moderate level of separation between the two clusters, supporting their distinct characteristics. Visualizations Two Distinct Patient Profiles Emerge from Cluster Analysis\nThis scatter plot visualizes the two distinct patient clusters identified using K-means clustering. The x-axis represents the length of the initial hospital stay (Initial_days), and the y-axis represents the average daily charge (TotalCharge).\nMean Values of Key Variables for Each Patient Cluster\nThis bar chart compares the standardized mean values (centroids) of key variables for the two patient clusters. The differences in centroids highlight the distinct characteristics of each group, such as shorter vs. longer hospital stays and lower vs. higher average charges.\nRecommendations Healthcare providers could use these cluster insights to develop targeted care strategies and resource allocation plans for each patient group. For example, patients in the cluster with shorter stays and lower charges might benefit from streamlined discharge processes and follow-up care to prevent readmissions. Patients in the cluster with longer stays and higher charges might require more intensive care coordination, disease management programs, or support services to address their complex needs. Link to Code Github Repository\n","description":"This project aimed to identify the factors influencing the likelihood of a patient being readmitted to the hospital within 30 days of discharge.","tags":["kmeans","scikit-learn"],"title":"Identifying Distinct Patient Groups Using K-means Clustering","uri":"/post/projects/kmeans/"},{"content":" Dimensionality Reduction of Patient Medical Data Using Principal Component Analysis (PCA) Description This project aimed to reduce the complexity and redundancy in a dataset of patient medical records while preserving essential information. Using Principal Component Analysis (PCA), a dimensionality reduction technique, I identified a smaller set of uncorrelated variables called “principal components” that captured the most significant variance in the data. This simplification facilitated more efficient analysis and visualization of the data’s underlying patterns.\nSkills Used Python, Pandas, NumPy, Scikit-learn, Principal Component Analysis (PCA), Data Standardization, Eigenvalue Analysis, Scree Plot Interpretation, Data Visualization, Communication of Results.\nKey Findings Four principal components were identified using the Kaiser Criterion (eigenvalues greater than 1.0) as significant. These four principal components explained 72.90% of the total variance in the original dataset, effectively reducing its dimensionality from eight original variables to four principal components. Analysis of the loading matrix revealed that “Initial_days” (length of initial hospital stay) and “TotalCharge” (total charges) were the most influential variables contributing to the first principal component, highlighting their significant role in explaining the variation within the dataset. Visualizations Explained Variance by Principal Component: Identifying the ‘Elbow’ Point This scree plot visually represents the explained variance by each principal component. The “elbow” point, where the explained variance levels off, indicates that four principal components are sufficient to capture most of the information in the data.\nLoading Matrix Heatmap: Visualizing Variable Contributions to Principal Components This heatmap [or table] displays the loadings of each original variable on the first four principal components. The loadings, represented by color intensity [or numerical values], indicate the strength and direction of the relationship between each variable and the corresponding principal component. The variables “Initial_days” and “TotalCharge” have the highest loadings on the first principal component (PC1), highlighting their significant contribution.\nRecommendations Focus on Key Variables: The analysis suggests that “Initial_days” and “TotalCharge” are the most influential variables in explaining the variance in this dataset. Further analysis could focus on understanding the relationship between these variables and exploring how they might influence other aspects of healthcare delivery. Simplified Data Representation: The reduced set of four principal components can be used in subsequent analyses or modeling tasks, simplifying the data while retaining essential information. Link to Code Github Repository\n","description":"This project aimed to reduce the complexity and redundancy in a dataset of patient medical records while preserving essential information. ","tags":["PCA","Eigenvalue"],"title":"Dimensionality Reduction of Patient Medical Data Using Principal Component Analysis (PCA)","uri":"/post/projects/pca/"},{"content":" Market Basket Analysis of Computer Accessory Purchases Description This project aimed to uncover purchasing patterns and associations between computer accessories using Market Basket Analysis (MBA). I applied the Apriori algorithm to a dataset of customer transactions from an online computer retailer. The goal was to identify items frequently purchased together, providing insights for product recommendations, store layouts, and marketing strategies to increase sales.\nSkills Used Python, Pandas, mlxtend Library (Apriori Algorithm, Association Rules), Data Cleaning, Data Transformation, Market Basket Analysis, Support, Confidence, Lift, Data Visualization, Interpretation of Association Rules.\nKey Findings The analysis revealed several strong association rules, indicating items frequently purchased together. The most prominent finding was the frequent co-purchase of “Dust-Off Compressed Gas 2 pack” with other items, such as iPhone charger cables, blue light blocking glasses, and SanDisk memory cards. This suggests customers often buy cleaning supplies alongside new accessories. The association rule “(10ft iPhone Charger Cable 2 Pack) -\u003e (Dust-Off Compressed Gas 2 pack)” had a lift of 1.91, indicating that customers who purchase the charger cable are 1.91 times more likely to also purchase the compressed gas.\nVisualizations Top Association Rules for Computer Accessory Purchases Comparing Support, Confidence, and Lift for Top Rules This grouped bar chart compares the support, confidence, and lift values for the top five association rules, as detailed in the table above. Each group of bars represents a numbered rule, allowing for a clear visual comparison of the strength of each association.\nRecommendations Product Bundling and Promotions: Offer bundles or discounts for products that are frequently purchased together, such as the “10ft iPhone Charger Cable 2 Pack” and the “Dust-Off Compressed Gas 2 pack.” Targeted Recommendations: Implement a recommendation engine that suggests “Dust-Off Compressed Gas 2 pack” or other relevant accessories to customers purchasing items like iPhone chargers, blue light blocking glasses, or memory cards. Strategic Product Placement: Place frequently co-purchased items near each other in physical store layouts or on website product pages to encourage additional sales. Link to Code Github Repository\n","description":"This project aimed to uncover purchasing patterns and associations between computer accessories using Market Basket Analysis (MBA).","tags":["MBA","data cleaning"],"title":"Market Basket Analysis of Computer Accessory Purchases","uri":"/post/projects/mba/"},{"content":" Data Cleaning and Principal Component Analysis of Patient Medical Records Description This project focused on preparing a dataset of 10,000 patient medical records for analysis. The data contained missing values, inconsistencies, and potential outliers that needed to be addressed to ensure data quality and accurate insights. I applied various data cleaning techniques in Python, including imputation, data type conversion, and outlier detection. Following the cleaning process, I performed Principal Component Analysis (PCA) to identify the most influential variables and reduce the dataset’s dimensionality.\nSkills Used Python, Pandas, NumPy, Data Cleaning, Missing Value Imputation, Outlier Detection, Data Transformation, Principal Component Analysis (PCA), Eigenvalue Analysis, Scree Plot Interpretation, Data Visualization, Communication of Results\nKey Findings Effective Data Cleaning: Identified and addressed missing values in seven variables using appropriate imputation techniques based on the data distribution (mean, median, and random sampling based on value ratios). Re-expressed categorical variables (“Overweight” and “Anxiety”) to ensure consistency. Outlier Handling: Detected outliers in several variables but chose to retain them, recognizing their potential importance and emphasizing the need for consultation with stakeholders before removing potentially valuable data. Dimensionality Reduction with PCA: Applied PCA and identified six principal components with eigenvalues greater than 1.0, explaining a significant portion of the variance in the dataset. The first two components (PC1 and PC2) were found to be the most influential. Insightful Variable Identification: Analysis of the PCA loadings revealed that “TotalCharge,” “VitD_levels,” “Initial_days,” “Additional_charges,” and “Age” were the most significant contributors to PC1 and PC2, highlighting these variables as potentially crucial for further investigation. Visualizations Scree Plot: Finding the “Elbow” to Determine Component Retention This scree plot shows the eigenvalues for each principal component. The elbow point at the third component suggests that the first two components (PC1 and PC2) capture most of the variation in the data.\nHistogram of Age Before and After Cleaning Recommendations Consult with Stakeholders on Outliers: While outliers were retained in this analysis, it’s essential to engage with subject-matter experts or stakeholders to determine the appropriate handling of these data points. Their insights could reveal valuable information or potential data entry errors. Focus on Key Variables: The PCA results suggest that variables like “TotalCharge,” “Initial_days,” and “Age” are particularly influential in understanding the variance in patient data. Further investigations should focus on analyzing these variables in more detail and their potential impact on healthcare outcomes. Data Collection and Validation: To improve data quality and reduce missing values in future data collection efforts, implement stricter validation rules during data entry and ensure consistency in data recording practices. Link to Code Code Detection Github Repository\nCode Treatment Github Repository\n","description":"This project focused on preparing a dataset of 10,000 patient medical records for analysis.","tags":["eigenvalue","PCA","data cleaning"],"title":"Data Cleaning and Principal Component Analysis of Patient Medical Records","uri":"/post/projects/cleaning-pca/"},{"content":" Exploring the Relationship Between Vitamin D Levels and Hospital Readmissions Description This project investigated whether a relationship exists between a patient’s Vitamin D levels and their likelihood of being readmitted to the hospital within 30 days of discharge. Using a dataset of 10,000 patient records, I conducted an exploratory data analysis (EDA) to understand the data’s characteristics and then performed a two-sample independent t-test to test the hypothesis that there is a significant difference in Vitamin D levels between patients who were readmitted and those who were not.\nSkills Used Python, Pandas, NumPy, Scipy.stats, Data Exploration, Data Visualization (Histograms, Bar Charts, Density Plots, Scatterplots), Descriptive Statistics, Hypothesis Testing (T-test), Interpretation of Results, Communication of Findings.\nKey Findings Exploratory data analysis revealed that the Vitamin D levels for both groups (readmitted and not readmitted) followed a normal distribution and had similar variances, meeting the assumptions of the t-test. The independent two-sample t-test yielded a p-value of 0.68, which is greater than the significance level of 0.05. Therefore, the null hypothesis (no significant difference in Vitamin D levels) could not be rejected. The analysis did not find a statistically significant association between Vitamin D levels and hospital readmission rates in this dataset. Visualizations Vitamin D Levels and Hospital Readmission: No Clear Association This density plot compares the distributions of Vitamin D levels for patients who were readmitted to the hospital within 30 days and those who were not. The distributions are similar, suggesting that Vitamin D levels are not a strong factor in predicting readmissions.\nNo Strong Correlation Observed Between Vitamin D and Income This scatterplot explores the relationship between Vitamin D levels and patient income. The lack of a clear pattern suggests that these variables are not strongly correlated.\nRecommendations Investigate Other Variables: Since Vitamin D levels did not show a significant association with readmissions, further exploration of other potential predictors within the dataset is recommended. Collect Time-Related Data: Including a date or timestamp associated with hospital admissions and readmissions could provide valuable insights into temporal trends and allow for more sophisticated time-based analysis. Compare Pre- and Post-Readmission Data: If possible, analyze data from the same patients before and after readmission to identify changes in their health metrics or treatment plans that might contribute to readmission risk. Link to Code Github Repository\n","description":"This project investigated whether a relationship exists between a patient's Vitamin D levels and their likelihood of being readmitted to the hospital within 30 days of discharge.","tags":["EDA","scipy.stats"],"title":"Exploring the Relationship Between Vitamin D Levels and Hospital Readmissions","uri":"/post/projects/eda-vitd/"},{"content":"Articles about learn in public, brag document and free stuffs.\nLearn In Public If there’s a golden rule, it’s this one, so I put it first. All the other rules are more or less elaborations of this rule #1.\nA habit of creating learning exhaust:\nWrite blogs and tutorials and cheatsheets. Speak at meetups and conferences. Ask and answer things on Stackoverflow or Reddit. Avoid the walled gardens like Slack and Discord, they’re not public. Make Youtube videos or Twitch streams. Start a newsletter. Draw cartoons. Link Make Free Stuff The best growth hack is still to build something people enjoy, then attaching no strings to it. You’d be surprised how far that can get you.\nMake free stuff! The web is still for everyone.\nLink\nHow to hone your new superpower: teaching I learned early in my developer journey that teaching others is an effective way to quickly deepen my understanding of a new concept or technology. I’ve found that needing to articulate a particular concept to others causes me to revisit my assumptions and leads me to do additional research to fill any knowledge gaps.\nLink\nYour future self will thank you: Building your personal documentation. Developers can take a DRY approach to how they search for answers to questions they encounter multiple times. By relying on an internal database (or “second brain”) they can reduce their reliance on external search engines.\nLink\nBrag now, remember later: Document your accomplishments Given five minutes notice to summarize your recent professional and personal accomplishments and wins, how detailed would your response be? Would that be enough time for you to sufficiently capture some of the things you’re most proud of from the past few months or years?\nLink\nThe most successful developers share more than they take One of the questions I always ask successful bloggers is: what motivated you to start? The answer is always the same: I did it for myself. Whatever your work, you should embrace the philosophy of “public by default”.\nPublic-by-default means this: everytime you create something, learn something, or just notice something’s interesting, do it in public. This may seem daunting—writing blog posts, helping the community and transforming ideas from thoughts into words all takes time. But sharing is like a muscle, and by committing to a regular schedule, you become much more efficient. This consistency of volume is also key to reaping the benefits of sharing.\nTo truly embrace public-by-default, it’s not enough to share your successful projects and knowledge, but additionally to bring the humility to share your learning and failures.\nLink\n","description":"Articles about learning in public, brag document and free stuffs.","tags":["newsletter"],"title":"Collections No.1","uri":"/collections/2023/collections/"},{"content":"Articles about learn in public, brag document and free stuffs.\nLearn In Public If there’s a golden rule, it’s this one, so I put it first. All the other rules are more or less elaborations of this rule #1.\nA habit of creating learning exhaust:\nWrite blogs and tutorials and cheatsheets. Speak at meetups and conferences. Ask and answer things on Stackoverflow or Reddit. Avoid the walled gardens like Slack and Discord, they’re not public. Make Youtube videos or Twitch streams. Start a newsletter. Draw cartoons. Link Make Free Stuff The best growth hack is still to build something people enjoy, then attaching no strings to it. You’d be surprised how far that can get you.\nMake free stuff! The web is still for everyone.\nLink\nHow to hone your new superpower: teaching I learned early in my developer journey that teaching others is an effective way to quickly deepen my understanding of a new concept or technology. I’ve found that needing to articulate a particular concept to others causes me to revisit my assumptions and leads me to do additional research to fill any knowledge gaps.\nLink\nYour future self will thank you: Building your personal documentation. Developers can take a DRY approach to how they search for answers to questions they encounter multiple times. By relying on an internal database (or “second brain”) they can reduce their reliance on external search engines.\nLink\nBrag now, remember later: Document your accomplishments Given five minutes notice to summarize your recent professional and personal accomplishments and wins, how detailed would your response be? Would that be enough time for you to sufficiently capture some of the things you’re most proud of from the past few months or years?\nLink\nThe most successful developers share more than they take One of the questions I always ask successful bloggers is: what motivated you to start? The answer is always the same: I did it for myself. Whatever your work, you should embrace the philosophy of “public by default”.\nPublic-by-default means this: everytime you create something, learn something, or just notice something’s interesting, do it in public. This may seem daunting—writing blog posts, helping the community and transforming ideas from thoughts into words all takes time. But sharing is like a muscle, and by committing to a regular schedule, you become much more efficient. This consistency of volume is also key to reaping the benefits of sharing.\nTo truly embrace public-by-default, it’s not enough to share your successful projects and knowledge, but additionally to bring the humility to share your learning and failures.\nLink\n","description":"Articles about learning in public, brag document and free stuffs.","tags":["newsletter"],"title":"Collections No.2","uri":"/collections/2023/collections2/"},{"content":"Articles about learn in public, brag document and free stuffs.\nLearn In Public If there’s a golden rule, it’s this one, so I put it first. All the other rules are more or less elaborations of this rule #1.\nA habit of creating learning exhaust:\nWrite blogs and tutorials and cheatsheets. Speak at meetups and conferences. Ask and answer things on Stackoverflow or Reddit. Avoid the walled gardens like Slack and Discord, they’re not public. Make Youtube videos or Twitch streams. Start a newsletter. Draw cartoons. Link Make Free Stuff The best growth hack is still to build something people enjoy, then attaching no strings to it. You’d be surprised how far that can get you.\nMake free stuff! The web is still for everyone.\nLink\nHow to hone your new superpower: teaching I learned early in my developer journey that teaching others is an effective way to quickly deepen my understanding of a new concept or technology. I’ve found that needing to articulate a particular concept to others causes me to revisit my assumptions and leads me to do additional research to fill any knowledge gaps.\nLink\nYour future self will thank you: Building your personal documentation. Developers can take a DRY approach to how they search for answers to questions they encounter multiple times. By relying on an internal database (or “second brain”) they can reduce their reliance on external search engines.\nLink\nBrag now, remember later: Document your accomplishments Given five minutes notice to summarize your recent professional and personal accomplishments and wins, how detailed would your response be? Would that be enough time for you to sufficiently capture some of the things you’re most proud of from the past few months or years?\nLink\nThe most successful developers share more than they take One of the questions I always ask successful bloggers is: what motivated you to start? The answer is always the same: I did it for myself. Whatever your work, you should embrace the philosophy of “public by default”.\nPublic-by-default means this: everytime you create something, learn something, or just notice something’s interesting, do it in public. This may seem daunting—writing blog posts, helping the community and transforming ideas from thoughts into words all takes time. But sharing is like a muscle, and by committing to a regular schedule, you become much more efficient. This consistency of volume is also key to reaping the benefits of sharing.\nTo truly embrace public-by-default, it’s not enough to share your successful projects and knowledge, but additionally to bring the humility to share your learning and failures.\nLink\n","description":"Articles about learning in public, brag document and free stuffs.","tags":["newsletter"],"title":"Collections No.3","uri":"/collections/2023/collections3/"},{"content":"Articles about learn in public, brag document and free stuffs.\nLearn In Public If there’s a golden rule, it’s this one, so I put it first. All the other rules are more or less elaborations of this rule #1.\nA habit of creating learning exhaust:\nWrite blogs and tutorials and cheatsheets. Speak at meetups and conferences. Ask and answer things on Stackoverflow or Reddit. Avoid the walled gardens like Slack and Discord, they’re not public. Make Youtube videos or Twitch streams. Start a newsletter. Draw cartoons. Link Make Free Stuff The best growth hack is still to build something people enjoy, then attaching no strings to it. You’d be surprised how far that can get you.\nMake free stuff! The web is still for everyone.\nLink\nHow to hone your new superpower: teaching I learned early in my developer journey that teaching others is an effective way to quickly deepen my understanding of a new concept or technology. I’ve found that needing to articulate a particular concept to others causes me to revisit my assumptions and leads me to do additional research to fill any knowledge gaps.\nLink\nYour future self will thank you: Building your personal documentation. Developers can take a DRY approach to how they search for answers to questions they encounter multiple times. By relying on an internal database (or “second brain”) they can reduce their reliance on external search engines.\nLink\nBrag now, remember later: Document your accomplishments Given five minutes notice to summarize your recent professional and personal accomplishments and wins, how detailed would your response be? Would that be enough time for you to sufficiently capture some of the things you’re most proud of from the past few months or years?\nLink\nThe most successful developers share more than they take One of the questions I always ask successful bloggers is: what motivated you to start? The answer is always the same: I did it for myself. Whatever your work, you should embrace the philosophy of “public by default”.\nPublic-by-default means this: everytime you create something, learn something, or just notice something’s interesting, do it in public. This may seem daunting—writing blog posts, helping the community and transforming ideas from thoughts into words all takes time. But sharing is like a muscle, and by committing to a regular schedule, you become much more efficient. This consistency of volume is also key to reaping the benefits of sharing.\nTo truly embrace public-by-default, it’s not enough to share your successful projects and knowledge, but additionally to bring the humility to share your learning and failures.\nLink\n","description":"Articles about learning in public, brag document and free stuffs.","tags":["newsletter"],"title":"Collections No.4","uri":"/collections/2023/collections4/"},{"content":"Articles about learn in public, brag document and free stuffs.\nLearn In Public If there’s a golden rule, it’s this one, so I put it first. All the other rules are more or less elaborations of this rule #1.\nA habit of creating learning exhaust:\nWrite blogs and tutorials and cheatsheets. Speak at meetups and conferences. Ask and answer things on Stackoverflow or Reddit. Avoid the walled gardens like Slack and Discord, they’re not public. Make Youtube videos or Twitch streams. Start a newsletter. Draw cartoons. Link Make Free Stuff The best growth hack is still to build something people enjoy, then attaching no strings to it. You’d be surprised how far that can get you.\nMake free stuff! The web is still for everyone.\nLink\nHow to hone your new superpower: teaching I learned early in my developer journey that teaching others is an effective way to quickly deepen my understanding of a new concept or technology. I’ve found that needing to articulate a particular concept to others causes me to revisit my assumptions and leads me to do additional research to fill any knowledge gaps.\nLink\nYour future self will thank you: Building your personal documentation. Developers can take a DRY approach to how they search for answers to questions they encounter multiple times. By relying on an internal database (or “second brain”) they can reduce their reliance on external search engines.\nLink\nBrag now, remember later: Document your accomplishments Given five minutes notice to summarize your recent professional and personal accomplishments and wins, how detailed would your response be? Would that be enough time for you to sufficiently capture some of the things you’re most proud of from the past few months or years?\nLink\nThe most successful developers share more than they take One of the questions I always ask successful bloggers is: what motivated you to start? The answer is always the same: I did it for myself. Whatever your work, you should embrace the philosophy of “public by default”.\nPublic-by-default means this: everytime you create something, learn something, or just notice something’s interesting, do it in public. This may seem daunting—writing blog posts, helping the community and transforming ideas from thoughts into words all takes time. But sharing is like a muscle, and by committing to a regular schedule, you become much more efficient. This consistency of volume is also key to reaping the benefits of sharing.\nTo truly embrace public-by-default, it’s not enough to share your successful projects and knowledge, but additionally to bring the humility to share your learning and failures.\nLink\n","description":"Articles about learning in public, brag document and free stuffs.","tags":["newsletter"],"title":"Collections No.5","uri":"/collections/2023/collections5/"},{"content":"Articles about learn in public, brag document and free stuffs.\nLearn In Public If there’s a golden rule, it’s this one, so I put it first. All the other rules are more or less elaborations of this rule #1.\nA habit of creating learning exhaust:\nWrite blogs and tutorials and cheatsheets. Speak at meetups and conferences. Ask and answer things on Stackoverflow or Reddit. Avoid the walled gardens like Slack and Discord, they’re not public. Make Youtube videos or Twitch streams. Start a newsletter. Draw cartoons. Link Make Free Stuff The best growth hack is still to build something people enjoy, then attaching no strings to it. You’d be surprised how far that can get you.\nMake free stuff! The web is still for everyone.\nLink\nHow to hone your new superpower: teaching I learned early in my developer journey that teaching others is an effective way to quickly deepen my understanding of a new concept or technology. I’ve found that needing to articulate a particular concept to others causes me to revisit my assumptions and leads me to do additional research to fill any knowledge gaps.\nLink\nYour future self will thank you: Building your personal documentation. Developers can take a DRY approach to how they search for answers to questions they encounter multiple times. By relying on an internal database (or “second brain”) they can reduce their reliance on external search engines.\nLink\nBrag now, remember later: Document your accomplishments Given five minutes notice to summarize your recent professional and personal accomplishments and wins, how detailed would your response be? Would that be enough time for you to sufficiently capture some of the things you’re most proud of from the past few months or years?\nLink\nThe most successful developers share more than they take One of the questions I always ask successful bloggers is: what motivated you to start? The answer is always the same: I did it for myself. Whatever your work, you should embrace the philosophy of “public by default”.\nPublic-by-default means this: everytime you create something, learn something, or just notice something’s interesting, do it in public. This may seem daunting—writing blog posts, helping the community and transforming ideas from thoughts into words all takes time. But sharing is like a muscle, and by committing to a regular schedule, you become much more efficient. This consistency of volume is also key to reaping the benefits of sharing.\nTo truly embrace public-by-default, it’s not enough to share your successful projects and knowledge, but additionally to bring the humility to share your learning and failures.\nLink\n","description":"Articles about learning in public, brag document and free stuffs.","tags":["newsletter"],"title":"Collections No.6","uri":"/collections/2023/collections6/"},{"content":"Journaling is not just a little thing you do to pass the time, to write down your memories—though it can be—it’s a strategy that has helped brilliant, powerful and wise people become better at what they do.\n","title":"Note 1","uri":"/notes/2020/change/"},{"content":"Journaling is not just a little thing you do to pass the time, to write down your memories—though it can be—it’s a strategy that has helped brilliant, powerful and wise people become better at what they do.\n","title":"Note 2","uri":"/notes/2020/change2/"},{"content":"Journaling is not just a little thing you do to pass the time, to write down your memories—though it can be—it’s a strategy that has helped brilliant, powerful and wise people become better at what they do.\n","title":"Note 3","uri":"/notes/2020/change3/"}]
